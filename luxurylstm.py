# -*- coding: utf-8 -*-
"""LuxuryLSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cg6fJqG6Y-gqIDNe8lZ_p3BNNOKmINjs
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
import matplotlib.pyplot as plt

class LuxuryLSTM:
    def __init__(self, sequence_length=4):
        self.sequence_length = sequence_length
        self.scaler_X = StandardScaler()
        self.scaler_y = StandardScaler()
        self.model = None
        self.feature_names = None

    def create_sequences(self, data, seq_length):
        X, y = [], []
        for i in range(len(data) - seq_length):
            X.append(data[i:(i + seq_length)])
            y.append(data[i + seq_length, 0])
        return np.array(X), np.array(y)

    def prepare_data(self, df):
        """Prepare LSTM training data."""
        selected_features = [
            'Growth_Rate',
            'GDP.Growth.Rate',
            'Gini.Change.Rate',
            'Top.1..Growth.Rate',
            'Top.10..Growth.Rate'
        ]

        self.feature_names = selected_features[1:]

        data = df[selected_features].values
        y = data[:, 0].reshape(-1, 1)
        X = data[:, 1:]

        X_scaled = self.scaler_X.fit_transform(X)
        y_scaled = self.scaler_y.fit_transform(y)
        combined_data = np.hstack((y_scaled, X_scaled))

        X_seq, y_seq = self.create_sequences(combined_data, self.sequence_length)
        return X_seq, y_seq

    def build_model(self, input_shape):
        """Build and compile the LSTM model."""
        model = Sequential([
            LSTM(64, input_shape=input_shape,
                 return_sequences=True,
                 kernel_regularizer=l2(0.01),
                 recurrent_regularizer=l2(0.01),
                 activation='tanh'),
            Dropout(0.3),
            LSTM(32,
                 kernel_regularizer=l2(0.01),
                 recurrent_regularizer=l2(0.01)),
            Dropout(0.3),
            Dense(16, activation='relu', kernel_regularizer=l2(0.01)),
            Dropout(0.2),
            Dense(1)
        ])

        model.compile(optimizer='adam',
                      loss='huber',
                      metrics=['mae'])

        self.model = model
        return model

    def train(self, X, y, epochs=150, batch_size=16, validation_split=0.2):
        """Train the LSTM model."""
        callbacks = [
            EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, mode='min'),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.0001, mode='min')
        ]

        history = self.model.fit(
            X, y,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=validation_split,
            callbacks=callbacks,
            verbose=1
        )

        return history

    def predict(self, X):
        """Predict and revert to original scale."""
        y_pred_scaled = self.model.predict(X)
        y_pred = self.scaler_y.inverse_transform(y_pred_scaled)
        return y_pred

    def evaluate_feature_importance(self, X, y):
        """Evaluate feature importance via permutation."""
        baseline_pred = self.predict(X)
        baseline_mse = np.mean((y - baseline_pred.flatten()) ** 2)

        importance_scores = {}
        for i, feature in enumerate(self.feature_names):
            X_shuffled = X.copy()
            X_shuffled[:, :, i+1] = np.random.permutation(X_shuffled[:, :, i+1])
            shuffled_pred = self.predict(X_shuffled)
            shuffled_mse = np.mean((y - shuffled_pred.flatten()) ** 2)
            importance = (shuffled_mse - baseline_mse) / baseline_mse
            importance_scores[feature] = importance

        return importance_scores

def plot_results(history, y_true, y_pred, importance_scores=None):
    """Visualize training history, predictions, and feature importance."""
    fig = plt.figure(figsize=(15, 12))

    ax1 = plt.subplot(3, 1, 1)
    ax1.plot(history.history['loss'], label='Training Loss')
    ax1.plot(history.history['val_loss'], label='Validation Loss')
    ax1.set_title('Training History')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()

    ax2 = plt.subplot(3, 1, 2)
    ax2.plot(y_true, label='Actual', alpha=0.7)
    ax2.plot(y_pred, label='Predicted', alpha=0.7)
    ax2.set_title('Luxury Market Growth Rate Prediction')
    ax2.set_xlabel('Time')
    ax2.set_ylabel('Growth Rate (%)')
    ax2.legend()

    if importance_scores:
        ax3 = plt.subplot(3, 1, 3)
        features = list(importance_scores.keys())
        scores = list(importance_scores.values())
        ax3.bar(features, scores)
        ax3.set_title('Feature Importance Scores')
        plt.xticks(rotation=45)
        ax3.set_ylabel('Importance Score')

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    df = pd.read_csv("merged_luxury_indicators.csv")

    model = LuxuryLSTM(sequence_length=4)
    X, y = model.prepare_data(df)

    input_shape = (X.shape[1], X.shape[2])
    lstm_model = model.build_model(input_shape)

    history = model.train(X, y)
    y_pred = model.predict(X)

    importance_scores = model.evaluate_feature_importance(X, y)
    print("\nFeature Importance Scores:")
    for feature, score in importance_scores.items():
        print(f"{feature}: {score:.4f}")

    mae = np.mean(np.abs(df['Growth_Rate'].values[model.sequence_length:] - y_pred.flatten()))
    mse = np.mean((df['Growth_Rate'].values[model.sequence_length:] - y_pred.flatten())**2)
    print(f"\nMean Absolute Error: {mae:.2f}")
    print(f"Mean Squared Error: {mse:.2f}")

    plot_results(history, df['Growth_Rate'].values[model.sequence_length:], y_pred.flatten(), importance_scores)